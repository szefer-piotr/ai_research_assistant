{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "# If you want to chunk PDF text, you can also import TextSplitter utilities:\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure OpenAI Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY= os.getenv('OPENAI_API_KEY')\n",
    "# In code, you might do:\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract all text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    text_content = []\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text_content.append(page.extract_text())\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "def extract_methodology_section(full_text: str) -> str:\n",
    "    \"\"\"\n",
    "    A naive approach to extract only the 'Methodology' section from\n",
    "    the full PDF text. Adjust to your own needs. \n",
    "    \"\"\"\n",
    "    # Example: Find the text from 'Methodology' heading to next heading like 'Results'\n",
    "    # This is very simplistic and might need to handle text structures properly.\n",
    "    start_keyword = \"Methodology\"\n",
    "    end_keywords = [\"Results\", \"Analysis\", \"Discussion\", \"Conclusion\"]\n",
    "    \n",
    "    start_index = full_text.lower().find(start_keyword.lower())\n",
    "    if start_index == -1:\n",
    "        return \"\"\n",
    "    \n",
    "    # Search for the earliest next section heading\n",
    "    subsequent_indices = []\n",
    "    for ek in end_keywords:\n",
    "        idx = full_text.lower().find(ek.lower(), start_index + len(start_keyword))\n",
    "        if idx != -1:\n",
    "            subsequent_indices.append(idx)\n",
    "    \n",
    "    if not subsequent_indices:\n",
    "        # If we don't find any subsequent heading, take everything after 'Methodology'\n",
    "        return full_text[start_index:]\n",
    "    \n",
    "    end_index = min(subsequent_indices)\n",
    "    return full_text[start_index:end_index]\n",
    "\n",
    "def generate_dataset_summary(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a summary for each column in the dataset:\n",
    "      - Numeric columns: min, max, mean, std, etc.\n",
    "      - Categorical columns: unique values, counts\n",
    "      - Date/Time columns: min date, max date\n",
    "    Return a dictionary containing the summary data.\n",
    "    \"\"\"\n",
    "    summary_dict = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_info = {}\n",
    "        col_data = df[col].dropna()\n",
    "        \n",
    "        # Try to convert to datetime - if works, treat as time column\n",
    "        try:\n",
    "            col_data_dt = pd.to_datetime(col_data, errors='raise')\n",
    "            # If conversion successful, assume time column\n",
    "            col_info[\"column_type\"] = \"datetime\"\n",
    "            col_info[\"time_span_start\"] = str(col_data_dt.min())\n",
    "            col_info[\"time_span_end\"] = str(col_data_dt.max())\n",
    "        except ValueError:\n",
    "            # Not date/time, proceed to numeric or categorical logic\n",
    "            if pd.api.types.is_numeric_dtype(col_data):\n",
    "                col_info[\"column_type\"] = \"numeric\"\n",
    "                col_info[\"count\"] = int(col_data.count())\n",
    "                col_info[\"mean\"] = float(col_data.mean())\n",
    "                col_info[\"std\"] = float(col_data.std())\n",
    "                col_info[\"min\"] = float(col_data.min())\n",
    "                col_info[\"max\"] = float(col_data.max())\n",
    "            else:\n",
    "                col_info[\"column_type\"] = \"categorical\"\n",
    "                uniques = col_data.unique()\n",
    "                col_info[\"unique_values\"] = [str(u) for u in uniques]\n",
    "                col_info[\"unique_value_count\"] = len(uniques)\n",
    "        \n",
    "        summary_dict[col] = col_info\n",
    "    \n",
    "    return summary_dict\n",
    "\n",
    "def dict_to_xml_summarization(methodology_summary: str,\n",
    "                              statistics_extraction: str,\n",
    "                              dataset_summary: dict) -> str:\n",
    "    \"\"\"\n",
    "    Create an XML string combining methodology summary, \n",
    "    extracted statistical analyses, and dataset summary.\n",
    "    \"\"\"\n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "    root = ET.Element(\"SummaryOutput\")\n",
    "    \n",
    "    # Methodology part\n",
    "    methodology_el = ET.SubElement(root, \"MethodologySummary\")\n",
    "    methodology_el.text = methodology_summary\n",
    "    \n",
    "    # Statistical analyses part\n",
    "    stats_el = ET.SubElement(root, \"StatisticalAnalyses\")\n",
    "    stats_el.text = statistics_extraction\n",
    "    \n",
    "    # Dataset Summary\n",
    "    data_el = ET.SubElement(root, \"DatasetSummary\")\n",
    "    for col, info in dataset_summary.items():\n",
    "        col_el = ET.SubElement(data_el, \"Column\", name=col)\n",
    "        for key, val in info.items():\n",
    "            sub_el = ET.SubElement(col_el, key)\n",
    "            if isinstance(val, list):\n",
    "                sub_el.text = \", \".join(val)\n",
    "            else:\n",
    "                sub_el.text = str(val)\n",
    "    \n",
    "    # Convert to string\n",
    "    return ET.tostring(root, encoding='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangChain LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_llm = OpenAI(\n",
    "    temperature=0.0,       # Low temperature for factual summarization\n",
    "    model_name=\"gpt-4o\"    # Hypothetical Summarization Model\n",
    ")\n",
    "\n",
    "planner_llm = OpenAI(\n",
    "    temperature=0.0,\n",
    "    model_name=\"o1\"    # Hypothetical Planner Model\n",
    ")\n",
    "\n",
    "executor_llm = OpenAI(\n",
    "    temperature=0.0,\n",
    "    model_name=\"o1-mini\"  # Hypothetical Executor Model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPTS\n",
    "methodology_prompt = PromptTemplate(\n",
    "    input_variables=[\"methodology_text\"],\n",
    "    template=(\n",
    "        \"You are a model specialized in summarizing methodology sections. \"\n",
    "        \"Read the following text delimited by triple backticks and provide:\\n\"\n",
    "        \"1) A concise summary of the methodology.\\n\"\n",
    "        \"2) Exactly which statistical analyses were performed.\\n\"\n",
    "        \"3) How data was obtained.\\n\"\n",
    "        \"```\\n{methodology_text}\\n```\"\n",
    "    )\n",
    ")\n",
    "\n",
    "planner_prompt = PromptTemplate(\n",
    "    input_variables=[\"methodology_summary\", \"dataset_summary\"],\n",
    "    template=(\n",
    "        \"You are a planning model. Based on the methodology summary and dataset summary, \"\n",
    "        \"plan a step-by-step routine (as programmatic pseudocode or structured steps) \"\n",
    "        \"to execute the identified statistical analyses on the full dataset.\\n\\n\"\n",
    "        \"Methodology Summary:\\n{methodology_summary}\\n\\n\"\n",
    "        \"Dataset Summary:\\n{dataset_summary}\\n\\n\"\n",
    "        \"Provide your plan in XML format, with each <Step> containing a structured explanation \"\n",
    "        \"of how to implement it programmatically.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "executor_prompt = PromptTemplate(\n",
    "    input_variables=[\"analysis_plan_xml\"],\n",
    "    template=(\n",
    "        \"You are an executor model specialized in generating R scripts for each step. \"\n",
    "        \"Given the plan in XML, do the following:\\n\"\n",
    "        \"1. Generate separate R scripts for each analysis step.\\n\"\n",
    "        \"2. Generate a single master R script that runs them all in a structured manner.\\n\"\n",
    "        \"Output your results clearly, indicating how the scripts should be saved.\\n\\n\"\n",
    "        \"Plan XML:\\n{analysis_plan_xml}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Read PDF and extract methodology ---\n",
    "pdf_path = \"data/poc_example_data/lazaro_et_al_2021_accessible.pdf\"\n",
    "full_text = extract_text_from_pdf(pdf_path)\n",
    "methodology_text = extract_methodology_section(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Step 2: Summarize the methodology section & extract stats info ---\n",
    "summarization_chain = LLMChain(llm=summarization_llm, prompt=methodology_prompt)\n",
    "summary_result = summarization_chain.run(methodology_text=methodology_text)\n",
    "\n",
    "# Here, you can parse the result if you need to separate:\n",
    "# 1) Short summary\n",
    "# 2) Statistical analyses\n",
    "# 3) Data acquisition\n",
    "#\n",
    "# For simplicity, assume the model returns them in a structured textual format.\n",
    "# We'll just keep the entire text as `summary_result` or do a simple split if needed.\n",
    "\n",
    "# You might do advanced parsing or instruct the model to produce JSON for simpler extraction.\n",
    "# In this example, let's assume the final LLM text includes all necessary info.\n",
    "\n",
    "# We'll store them as separate text fields, if the model outputs them in some delineated form:\n",
    "# For demonstration, let's pretend the model returns something like:\n",
    "# \"SUMMARY: <summary> ... </summary>\\nSTAT_ANALYSES: <analyses> ... </analyses>\\nDATA_OBTAINED: <data> ... </data>\"\n",
    "# We'll do naive splitting. Adjust to your actual structure:\n",
    "\n",
    "summary_text = \"\"\n",
    "stat_analyses_text = \"\"\n",
    "data_obtained_text = \"\"\n",
    "\n",
    "# A naive parse:\n",
    "lines = summary_result.split(\"\\n\")\n",
    "current_section = None\n",
    "for line in lines:\n",
    "    line_upper = line.strip().upper()\n",
    "    if line_upper.startswith(\"SUMMARY:\"):\n",
    "        current_section = \"SUMMARY\"\n",
    "        summary_text = line.partition(\":\")[2].strip()\n",
    "    elif line_upper.startswith(\"STAT_ANALYSES:\"):\n",
    "        current_section = \"STAT_ANALYSES\"\n",
    "        stat_analyses_text = line.partition(\":\")[2].strip()\n",
    "    elif line_upper.startswith(\"DATA_OBTAINED:\"):\n",
    "        current_section = \"DATA_OBTAINED\"\n",
    "        data_obtained_text = line.partition(\":\")[2].strip()\n",
    "    else:\n",
    "        if current_section == \"SUMMARY\":\n",
    "            summary_text += \" \" + line\n",
    "        elif current_section == \"STAT_ANALYSES\":\n",
    "            stat_analyses_text += \" \" + line\n",
    "        elif current_section == \"DATA_OBTAINED\":\n",
    "            data_obtained_text += \" \" + line\n",
    "\n",
    "# --- Step 3: Load dataset and produce summary ---\n",
    "# Suppose you have a CSV or XLSX\n",
    "data_path = \"path/to/your/dataset.csv\"  # or .xlsx\n",
    "df = pd.read_csv(data_path)  # if XLSX -> pd.read_excel(data_path)\n",
    "\n",
    "dataset_summary_dict = generate_dataset_summary(df)\n",
    "\n",
    "# --- Step 4: Create the combined XML summarization ---\n",
    "# Combine all partial texts into single string for \"statistical analyses\" \n",
    "# since we have it from stat_analyses_text and data_obtained_text \n",
    "combined_stats_info = f\"Statistical analyses: {stat_analyses_text}\\nData obtained: {data_obtained_text}\"\n",
    "\n",
    "final_summary_xml = dict_to_xml_summarization(\n",
    "    methodology_summary=summary_text,\n",
    "    statistics_extraction=combined_stats_info,\n",
    "    dataset_summary=dataset_summary_dict\n",
    ")\n",
    "\n",
    "# You now have an XML containing:\n",
    "# 1. Methodology summary\n",
    "# 2. Statistical analyses\n",
    "# 3. Dataset summary\n",
    "\n",
    "# Print or save your final XML\n",
    "print(\"=== METHODOLOGY & DATASET SUMMARY XML ===\")\n",
    "print(final_summary_xml)\n",
    "\n",
    "# --- Step 5: Request a plan from the Planner Model (gpt-o1) ---\n",
    "planner_chain = LLMChain(llm=planner_llm, prompt=planner_prompt)\n",
    "\n",
    "# We'll pass the summary_text (methodology) and a short version of the dataset summary\n",
    "# You could also convert dataset_summary_dict to a textual representation\n",
    "dataset_summary_str = str(dataset_summary_dict)  # or build a more user-friendly text\n",
    "plan_result_xml = planner_chain.run(\n",
    "    methodology_summary=summary_text,\n",
    "    dataset_summary=dataset_summary_str\n",
    ")\n",
    "\n",
    "print(\"=== ANALYSIS PLAN (XML) ===\")\n",
    "print(plan_result_xml)\n",
    "\n",
    "# --- Step 6: Ask Executor Model to generate R code ---\n",
    "executor_chain = LLMChain(llm=executor_llm, prompt=executor_prompt)\n",
    "executor_result = executor_chain.run(analysis_plan_xml=plan_result_xml)\n",
    "\n",
    "# The result should contain instructions for multiple R scripts + a master script\n",
    "print(\"=== R CODE GENERATION ===\")\n",
    "print(executor_result)\n",
    "\n",
    "# You would then parse `executor_result` to extract each script’s content \n",
    "# and save them to `.R` files. For example:\n",
    "# parse out labeled sections like:\n",
    "#   <SCRIPT name=\"step1.R\"> ... </SCRIPT>\n",
    "#   <SCRIPT name=\"step2.R\"> ... </SCRIPT>\n",
    "#   <MASTER_SCRIPT> ... </MASTER_SCRIPT>\n",
    "# Then write to disk:\n",
    "#\n",
    "# with open(\"step1.R\", \"w\") as f:\n",
    "#     f.write(content_of_step1)\n",
    "# with open(\"main_analysis_pipeline.R\", \"w\") as f:\n",
    "#     f.write(master_script_content)\n",
    "#\n",
    "# Depending on how your LLM structure is returning them.\n",
    "\n",
    "print(\"=== WORKFLOW COMPLETE ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
