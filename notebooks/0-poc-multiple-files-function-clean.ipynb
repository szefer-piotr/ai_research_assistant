{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "def load_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Load all CSV files from a given folder into a dictionary.\n",
    "    Keys are file names and values are DataFrames.\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.'))\n",
    "    dataframes = {}\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            filename = os.path.basename(file)\n",
    "            dataframes[filename] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "# def load_excel_files(folder_path):\n",
    "#     \"\"\"\n",
    "#     Loads every XLSX file from a folder. If a file contains multiple sheets,\n",
    "#     each sheet is loaded as a separate dataset.\n",
    "\n",
    "#     Returns:\n",
    "#         datasets (list): A list of dictionaries with keys:\n",
    "#             'file'  - the file path,\n",
    "#             'sheet' - the sheet name,\n",
    "#             'data'  - the loaded DataFrame.\n",
    "#     \"\"\"\n",
    "#     file_pattern = os.path.join(folder_path, '*.xlsx')\n",
    "#     datasets = {}\n",
    "#     for file in glob.glob(file_pattern):\n",
    "#         try:\n",
    "#             xls = pd.ExcelFile(file)\n",
    "#             for sheet in xls.sheet_names:\n",
    "#                 try:\n",
    "#                     df = pd.read_excel(xls, sheet_name=sheet)\n",
    "#                     filename = os.path.basename(file)\n",
    "#                     datasets[filename] = {'file': file, 'sheet': sheet, 'data': df}\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file '{file}': {e}\")\n",
    "#     return datasets\n",
    "\n",
    "\n",
    "# def load_excel_files(folder_path):\n",
    "#     \"\"\"\n",
    "#     Loads every XLSX file from a folder. If a file contains multiple sheets,\n",
    "#     each sheet is loaded as a separate dataset.\n",
    "    \n",
    "#     Returns:\n",
    "#         datasets (dict): A dictionary where keys are file names and values are lists \n",
    "#         of dictionaries. Each inner dictionary has:\n",
    "#             'sheet' - the sheet name,\n",
    "#             'data'  - the loaded DataFrame.\n",
    "#     \"\"\"\n",
    "#     file_pattern = os.path.join(folder_path, '*.xlsx')\n",
    "#     datasets = {}\n",
    "#     for file in glob.glob(file_pattern):\n",
    "#         filename = os.path.basename(file)\n",
    "#         datasets[filename] = []  # Initialize list for multiple sheets\n",
    "#         try:\n",
    "#             xls = pd.ExcelFile(file)\n",
    "#             for sheet in xls.sheet_names:\n",
    "#                 try:\n",
    "#                     df = pd.read_excel(xls, sheet_name=sheet)\n",
    "#                     datasets[filename].append({'sheet': sheet, 'data': df})\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file '{file}': {e}\")\n",
    "#     return datasets\n",
    "\n",
    "def load_excel_files(folder_path):\n",
    "    \"\"\"\n",
    "    Loads every XLSX file from a folder. If a file contains multiple sheets,\n",
    "    each sheet is loaded as a separate dataset.\n",
    "    \n",
    "    Returns:\n",
    "        datasets (dict): A dictionary where keys are file names and values are dictionaries.\n",
    "        Each inner dictionary has keys as sheet names and values as the loaded DataFrame.\n",
    "    \"\"\"\n",
    "    file_pattern = os.path.join(folder_path, '*.xlsx')\n",
    "    datasets = {}\n",
    "    for file in glob.glob(file_pattern):\n",
    "        filename = os.path.basename(file)\n",
    "        datasets[filename] = {}  # Initialize dictionary for sheets\n",
    "        try:\n",
    "            xls = pd.ExcelFile(file)\n",
    "            for sheet in xls.sheet_names:\n",
    "                try:\n",
    "                    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "                    datasets[filename][sheet] = df\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file '{file}': {e}\")\n",
    "    return datasets\n",
    "\n",
    "def get_candidate_signatures(df, threshold=100):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, return a mapping of candidate join column signatures.\n",
    "    \n",
    "    A candidate join column is one that is either of object type or has a limited\n",
    "    number of unique values (<= threshold). The signature is defined as a tuple:\n",
    "      (data type as string, frozenset of unique non-null values)\n",
    "    \n",
    "    Returns a dictionary mapping signature -> list of column names.\n",
    "    \"\"\"\n",
    "    candidate_signatures = {}\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        unique_vals = series.dropna().unique()\n",
    "        if len(unique_vals) == 0:\n",
    "            continue\n",
    "        # if the column is object type or has limited unique values, consider it candidate\n",
    "        if series.dtype == 'object' or len(unique_vals) <= threshold:\n",
    "            sig = (str(series.dtype), frozenset(unique_vals))\n",
    "            candidate_signatures.setdefault(sig, []).append(col)\n",
    "    return candidate_signatures\n",
    "\n",
    "def build_candidate_mapping(dataframes, threshold=100):\n",
    "    \"\"\"\n",
    "    Build a global mapping for candidate join columns.\n",
    "    \n",
    "    Returns a dictionary mapping:\n",
    "       candidate_signature -> list of tuples (file_name, column_name, unique_count)\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for file_name, df in dataframes.items():\n",
    "        cand_sig = get_candidate_signatures(df, threshold=threshold)\n",
    "        for sig, cols in cand_sig.items():\n",
    "            for col in cols:\n",
    "                unique_count = len(df[col].dropna().unique())\n",
    "                mapping.setdefault(sig, []).append((file_name, col, unique_count))\n",
    "    return mapping\n",
    "\n",
    "def build_join_graph(candidate_mapping):\n",
    "    \"\"\"\n",
    "    Build a graph where each node is a CSV file and an edge connects two files\n",
    "    if they share at least one candidate join column signature.\n",
    "    \n",
    "    The edge attribute 'candidate_keys' is a list of tuples:\n",
    "      (candidate_signature, column_in_file1, column_in_file2)\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    # add nodes (file names)\n",
    "    files = set()\n",
    "    for candidates in candidate_mapping.values():\n",
    "        for (file_name, col, _) in candidates:\n",
    "            files.add(file_name)\n",
    "    for file in files:\n",
    "        G.add_node(file)\n",
    "    # add edges for each candidate signature shared across files\n",
    "    for sig, candidates in candidate_mapping.items():\n",
    "        if len(candidates) > 1:\n",
    "            for i in range(len(candidates)):\n",
    "                for j in range(i+1, len(candidates)):\n",
    "                    file1, col1, _ = candidates[i]\n",
    "                    file2, col2, _ = candidates[j]\n",
    "                    if G.has_edge(file1, file2):\n",
    "                        G[file1][file2]['candidate_keys'].append((sig, col1, col2))\n",
    "                    else:\n",
    "                        G.add_edge(file1, file2, candidate_keys=[(sig, col1, col2)])\n",
    "    return G\n",
    "\n",
    "def find_common_join_key(df1, df2, threshold=100):\n",
    "    \"\"\"\n",
    "    Identify a common join key between two DataFrames.\n",
    "    \n",
    "    It computes candidate signatures for each and returns a tuple:\n",
    "      (column_name_in_df1, column_name_in_df2, candidate_signature)\n",
    "    for the first candidate signature that both dataframes share.\n",
    "    If no candidate is found, returns None.\n",
    "    \"\"\"\n",
    "    cand1 = get_candidate_signatures(df1, threshold)\n",
    "    cand2 = get_candidate_signatures(df2, threshold)\n",
    "    common_keys = set(cand1.keys()).intersection(set(cand2.keys()))\n",
    "    if common_keys:\n",
    "        # if several keys are common, choose one based on lower unique count in df1\n",
    "        best_key = None\n",
    "        best_count = None\n",
    "        for key in common_keys:\n",
    "            col1 = cand1[key][0]\n",
    "            col2 = cand2[key][0]\n",
    "            count1 = len(df1[col1].dropna().unique())\n",
    "            if best_count is None or count1 < best_count:\n",
    "                best_count = count1\n",
    "                best_key = (col1, col2, key)\n",
    "        return best_key\n",
    "    return None\n",
    "\n",
    "def join_dataframes(dataframes, candidate_mapping, threshold=100):\n",
    "    \"\"\"\n",
    "    Using the candidate mapping, build a join graph and for each connected component,\n",
    "    iteratively join the dataframes. For each connected component, the file with the fewest\n",
    "    rows (a proxy for lowest resolution) is chosen as the base.\n",
    "    \n",
    "    Returns a dictionary where keys are base file names (or component identifiers) and\n",
    "    values are the merged DataFrames.\n",
    "    \"\"\"\n",
    "    G = build_join_graph(candidate_mapping)\n",
    "    components = list(nx.connected_components(G))\n",
    "    merged_dfs = {}\n",
    "    processed_files = set()\n",
    "    \n",
    "    for comp in components:\n",
    "        if len(comp) == 1:\n",
    "            # No join possible; keep the file as is.\n",
    "            file = list(comp)[0]\n",
    "            merged_dfs[file] = dataframes[file]\n",
    "            processed_files.add(file)\n",
    "        else:\n",
    "            comp_files = list(comp)\n",
    "            # choose base file as one with the smallest number of rows\n",
    "            base_file = min(comp_files, key=lambda f: len(dataframes[f]))\n",
    "            merged_df = dataframes[base_file]\n",
    "            merged_files = {base_file}\n",
    "            remaining_files = set(comp_files) - merged_files\n",
    "\n",
    "            # Iteratively try to join any remaining file with the merged dataframe\n",
    "            while remaining_files:\n",
    "                joined = False\n",
    "                for file in list(remaining_files):\n",
    "                    join_key = find_common_join_key(merged_df, dataframes[file], threshold)\n",
    "                    if join_key:\n",
    "                        col_merged, col_new, sig = join_key\n",
    "                        df_to_join = dataframes[file].copy()\n",
    "                        # if the join column names differ, rename the new dataframe’s column\n",
    "                        if col_new != col_merged:\n",
    "                            df_to_join = df_to_join.rename(columns={col_new: col_merged})\n",
    "                        # outer join to preserve information\n",
    "                        merged_df = pd.merge(merged_df, df_to_join, on=col_merged, how='outer',\n",
    "                                             suffixes=('', '_' + file))\n",
    "                        merged_files.add(file)\n",
    "                        remaining_files.remove(file)\n",
    "                        joined = True\n",
    "                        break\n",
    "                if not joined:\n",
    "                    # If no join key is found for any remaining file, leave them separate.\n",
    "                    break\n",
    "            # Record the merged DataFrame from this component.\n",
    "            merged_dfs[base_file] = merged_df\n",
    "            processed_files.update(comp)\n",
    "    \n",
    "    # For any files not connected in the join graph, add them as individual outputs.\n",
    "    for file in dataframes:\n",
    "        if file not in processed_files:\n",
    "            merged_dfs[file] = dataframes[file]\n",
    "    return merged_dfs\n",
    "\n",
    "def process_csv_folder(folder_path, threshold=100):\n",
    "    \"\"\"\n",
    "    Wrapper function that:\n",
    "      1. Loads all CSV files from the given folder.\n",
    "      2. Analyzes each DataFrame’s columns for candidate join keys.\n",
    "      3. Identifies join relationships and builds a join graph.\n",
    "      4. Chooses a base dataset (lowest resolution) in each joinable group.\n",
    "      5. Performs iterative joins and returns a minimal set of merged DataFrames.\n",
    "      \n",
    "    The 'threshold' parameter controls the maximum unique value count for a column to be\n",
    "    considered as a candidate join column.\n",
    "    \n",
    "    Returns a dictionary of DataFrames keyed by an identifier.\n",
    "    \"\"\"\n",
    "    dataframes = load_csv_files(folder_path)\n",
    "    candidate_mapping = build_candidate_mapping(dataframes, threshold)\n",
    "    merged_dfs = join_dataframes(dataframes, candidate_mapping, threshold)\n",
    "    return merged_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../data/poc_data_and_similar_paper/\"\n",
    "excel_files = load_excel_files(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ecological traits of wild bees</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Species code</td>\n",
       "      <td>Family</td>\n",
       "      <td>Social behavior</td>\n",
       "      <td>Nesting place</td>\n",
       "      <td>Floral specificity</td>\n",
       "      <td>Flight beginning period</td>\n",
       "      <td>End of flight period</td>\n",
       "      <td>Lifespan [month]</td>\n",
       "      <td>Voltinism</td>\n",
       "      <td>Pollen carrying-structure</td>\n",
       "      <td>Mean body size</td>\n",
       "      <td>Rarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and_alf</td>\n",
       "      <td>Andrenidae</td>\n",
       "      <td>solit</td>\n",
       "      <td>soil</td>\n",
       "      <td>poly</td>\n",
       "      <td>May/June</td>\n",
       "      <td>July/August</td>\n",
       "      <td>4</td>\n",
       "      <td>biv</td>\n",
       "      <td>scopa on leg</td>\n",
       "      <td>small</td>\n",
       "      <td>rare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and_bar</td>\n",
       "      <td>Andrenidae</td>\n",
       "      <td>solit</td>\n",
       "      <td>soil</td>\n",
       "      <td>poly</td>\n",
       "      <td>March/April</td>\n",
       "      <td>July/August</td>\n",
       "      <td>4</td>\n",
       "      <td>biv</td>\n",
       "      <td>scopa on leg</td>\n",
       "      <td>medium</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and_bic</td>\n",
       "      <td>Andrenidae</td>\n",
       "      <td>solit</td>\n",
       "      <td>soil</td>\n",
       "      <td>poly</td>\n",
       "      <td>March/April</td>\n",
       "      <td>July/August</td>\n",
       "      <td>5</td>\n",
       "      <td>biv</td>\n",
       "      <td>scopa on leg</td>\n",
       "      <td>medium</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>sph_ruf</td>\n",
       "      <td>Halictidae</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>May/June</td>\n",
       "      <td>July/August</td>\n",
       "      <td>4</td>\n",
       "      <td>mon</td>\n",
       "      <td>clep</td>\n",
       "      <td>medium</td>\n",
       "      <td>rare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>sph_sca</td>\n",
       "      <td>Halictidae</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>May/June</td>\n",
       "      <td>July/August</td>\n",
       "      <td>4</td>\n",
       "      <td>mon</td>\n",
       "      <td>clep</td>\n",
       "      <td>medium</td>\n",
       "      <td>rare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>ste_bre</td>\n",
       "      <td>Megachilidae</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>May/June</td>\n",
       "      <td>July/August</td>\n",
       "      <td>3</td>\n",
       "      <td>mon</td>\n",
       "      <td>clep</td>\n",
       "      <td>small</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>ste_pun</td>\n",
       "      <td>Megachilidae</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>May/June</td>\n",
       "      <td>July/August</td>\n",
       "      <td>3</td>\n",
       "      <td>mon</td>\n",
       "      <td>clep</td>\n",
       "      <td>medium</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>thy_orb</td>\n",
       "      <td>Apidae</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>clep</td>\n",
       "      <td>July/August</td>\n",
       "      <td>July/August</td>\n",
       "      <td>2</td>\n",
       "      <td>mon</td>\n",
       "      <td>clep</td>\n",
       "      <td>medium</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ecological traits of wild bees    Unnamed: 1       Unnamed: 2  \\\n",
       "0                              NaN           NaN              NaN   \n",
       "1                     Species code        Family  Social behavior   \n",
       "2                          and_alf    Andrenidae            solit   \n",
       "3                          and_bar    Andrenidae            solit   \n",
       "4                          and_bic    Andrenidae            solit   \n",
       "..                             ...           ...              ...   \n",
       "185                        sph_ruf    Halictidae             clep   \n",
       "186                        sph_sca    Halictidae             clep   \n",
       "187                        ste_bre  Megachilidae             clep   \n",
       "188                        ste_pun  Megachilidae             clep   \n",
       "189                        thy_orb        Apidae             clep   \n",
       "\n",
       "        Unnamed: 3          Unnamed: 4                Unnamed: 5  \\\n",
       "0              NaN                 NaN                       NaN   \n",
       "1    Nesting place  Floral specificity  Flight beginning period    \n",
       "2             soil                poly                  May/June   \n",
       "3             soil                poly               March/April   \n",
       "4             soil                poly               March/April   \n",
       "..             ...                 ...                       ...   \n",
       "185           clep                clep                  May/June   \n",
       "186           clep                clep                  May/June   \n",
       "187           clep                clep                  May/June   \n",
       "188           clep                clep                  May/June   \n",
       "189           clep                clep               July/August   \n",
       "\n",
       "                Unnamed: 6        Unnamed: 7  Unnamed: 8  \\\n",
       "0                      NaN               NaN         NaN   \n",
       "1    End of flight period   Lifespan [month]  Voltinism    \n",
       "2              July/August                 4         biv   \n",
       "3              July/August                 4         biv   \n",
       "4              July/August                 5         biv   \n",
       "..                     ...               ...         ...   \n",
       "185            July/August                 4         mon   \n",
       "186            July/August                 4         mon   \n",
       "187            July/August                 3         mon   \n",
       "188            July/August                 3         mon   \n",
       "189            July/August                 2         mon   \n",
       "\n",
       "                    Unnamed: 9     Unnamed: 10 Unnamed: 11  \n",
       "0                          NaN             NaN         NaN  \n",
       "1    Pollen carrying-structure  Mean body size      Rarity  \n",
       "2                 scopa on leg           small        rare  \n",
       "3                 scopa on leg          medium      common  \n",
       "4                 scopa on leg          medium      common  \n",
       "..                         ...             ...         ...  \n",
       "185                       clep          medium        rare  \n",
       "186                       clep          medium        rare  \n",
       "187                       clep           small      common  \n",
       "188                       clep          medium      common  \n",
       "189                       clep          medium      common  \n",
       "\n",
       "[190 rows x 12 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_files[\"fourth_corner_functional_diversity.xlsx\"][\"wild_bee_traits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m excel_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "for item in excel_files.items():\n",
    "    print(item.keys())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_candidate_mapping(excel_files[\"fourth_corner_functional_diversity.xlsx\"][\"wild_bee_traits\"], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
