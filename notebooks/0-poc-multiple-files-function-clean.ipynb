{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# def load_csv_files(folder_path):\n",
    "#     \"\"\"\n",
    "#     Load all CSV files from a given folder into a dictionary.\n",
    "#     Keys are file names and values are DataFrames.\n",
    "#     \"\"\"\n",
    "#     csv_files = glob.glob(os.path.join(folder_path, '*.'))\n",
    "#     dataframes = {}\n",
    "#     for file in csv_files:\n",
    "#         try:\n",
    "#             df = pd.read_csv(file)\n",
    "#             filename = os.path.basename(file)\n",
    "#             dataframes[filename] = df\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading {file}: {e}\")\n",
    "#     return dataframes\n",
    "\n",
    "# def load_excel_files(folder_path):\n",
    "#     \"\"\"\n",
    "#     Loads every XLSX file from a folder. If a file contains multiple sheets,\n",
    "#     each sheet is loaded as a separate dataset.\n",
    "\n",
    "#     Returns:\n",
    "#         datasets (list): A list of dictionaries with keys:\n",
    "#             'file'  - the file path,\n",
    "#             'sheet' - the sheet name,\n",
    "#             'data'  - the loaded DataFrame.\n",
    "#     \"\"\"\n",
    "#     file_pattern = os.path.join(folder_path, '*.xlsx')\n",
    "#     datasets = {}\n",
    "#     for file in glob.glob(file_pattern):\n",
    "#         try:\n",
    "#             xls = pd.ExcelFile(file)\n",
    "#             for sheet in xls.sheet_names:\n",
    "#                 try:\n",
    "#                     df = pd.read_excel(xls, sheet_name=sheet)\n",
    "#                     filename = os.path.basename(file)\n",
    "#                     datasets[filename] = {'file': file, 'sheet': sheet, 'data': df}\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file '{file}': {e}\")\n",
    "#     return datasets\n",
    "\n",
    "\n",
    "# def load_excel_files(folder_path):\n",
    "#     \"\"\"\n",
    "#     Loads every XLSX file from a folder. If a file contains multiple sheets,\n",
    "#     each sheet is loaded as a separate dataset.\n",
    "    \n",
    "#     Returns:\n",
    "#         datasets (dict): A dictionary where keys are file names and values are lists \n",
    "#         of dictionaries. Each inner dictionary has:\n",
    "#             'sheet' - the sheet name,\n",
    "#             'data'  - the loaded DataFrame.\n",
    "#     \"\"\"\n",
    "#     file_pattern = os.path.join(folder_path, '*.xlsx')\n",
    "#     datasets = {}\n",
    "#     for file in glob.glob(file_pattern):\n",
    "#         filename = os.path.basename(file)\n",
    "#         datasets[filename] = []  # Initialize list for multiple sheets\n",
    "#         try:\n",
    "#             xls = pd.ExcelFile(file)\n",
    "#             for sheet in xls.sheet_names:\n",
    "#                 try:\n",
    "#                     df = pd.read_excel(xls, sheet_name=sheet)\n",
    "#                     datasets[filename].append({'sheet': sheet, 'data': df})\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file '{file}': {e}\")\n",
    "#     return datasets\n",
    "\n",
    "def load_excel_files(folder_path):\n",
    "    \"\"\"\n",
    "    Loads every XLSX file from a folder. If a file contains multiple sheets,\n",
    "    each sheet is loaded as a separate dataset.\n",
    "    \n",
    "    Returns:\n",
    "        datasets (dict): A dictionary where keys are file names and values are dictionaries.\n",
    "        Each inner dictionary has keys as sheet names and values as the loaded DataFrame.\n",
    "    \"\"\"\n",
    "    file_pattern = os.path.join(folder_path, '*.xlsx')\n",
    "    datasets = {}\n",
    "    for file in glob.glob(file_pattern):\n",
    "        filename = os.path.basename(file)\n",
    "        # datasets[filename] = {}  # Initialize dictionary for sheets\n",
    "        try:\n",
    "            xls = pd.ExcelFile(file)\n",
    "            for sheet in xls.sheet_names:\n",
    "                try:\n",
    "                    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "                    # datasets[filename][sheet] = df\n",
    "                    datasets[sheet] = df\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file '{file}': {e}\")\n",
    "    return datasets\n",
    "\n",
    "def get_candidate_signatures(df, threshold=100):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, return a mapping of candidate join column signatures.\n",
    "    \n",
    "    A candidate join column is one that is either of object type or has a limited\n",
    "    number of unique values (<= threshold). The signature is defined as a tuple:\n",
    "      (data type as string, frozenset of unique non-null values)\n",
    "    \n",
    "    Returns a dictionary mapping signature -> list of column names.\n",
    "    \"\"\"\n",
    "    candidate_signatures = {}\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        unique_vals = series.dropna().unique()\n",
    "        if len(unique_vals) == 0:\n",
    "            continue\n",
    "        # if the column is object type or has limited unique values, consider it candidate\n",
    "        if series.dtype == 'object' or len(unique_vals) <= threshold:\n",
    "            sig = (str(series.dtype), frozenset(unique_vals))\n",
    "            candidate_signatures.setdefault(sig, []).append(col)\n",
    "    return candidate_signatures\n",
    "\n",
    "\n",
    "def build_candidate_mapping(dataframes, threshold=100):\n",
    "    \"\"\"\n",
    "    Build a global mapping for candidate join columns.\n",
    "    \n",
    "    Returns a dictionary mapping:\n",
    "       candidate_signature -> list of tuples (file_name, column_name, unique_count)\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    \n",
    "    for sheet, df in dataframes.items():\n",
    "        print(sheet)\n",
    "        cand_sig = get_candidate_signatures(df, threshold=threshold)\n",
    "        for sig, cols in cand_sig.items():\n",
    "            for col in cols:\n",
    "                unique_count = len(df[col].dropna().unique())\n",
    "                mapping.setdefault(sig, []).append((sheet, col, unique_count)) \n",
    "\n",
    "    # for spreadsheet_name in dataframes.keys():\n",
    "    #     print(spreadsheet_name)\n",
    "    #     for sheet, df in dataframes[spreadsheet_name].items():\n",
    "    #         print(sheet)\n",
    "    #         cand_sig = get_candidate_signatures(df, threshold=threshold)\n",
    "    #         for sig, cols in cand_sig.items():\n",
    "    #             for col in cols:\n",
    "    #                 unique_count = len(df[col].dropna().unique())\n",
    "    #                 mapping.setdefault(sig, []).append((sheet, col, unique_count)) \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def build_join_graph(candidate_mapping):\n",
    "    \"\"\"\n",
    "    Build a graph where each node is a CSV file and an edge connects two files\n",
    "    if they share at least one candidate join column signature.\n",
    "    \n",
    "    The edge attribute 'candidate_keys' is a list of tuples:\n",
    "      (candidate_signature, column_in_file1, column_in_file2)\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    # add nodes (file names)\n",
    "    files = set()\n",
    "    for candidates in candidate_mapping.values():\n",
    "        for (file_name, col, _) in candidates:\n",
    "            files.add(file_name)\n",
    "    for file in files:\n",
    "        G.add_node(file)\n",
    "    # add edges for each candidate signature shared across files\n",
    "    for sig, candidates in candidate_mapping.items():\n",
    "        if len(candidates) > 1:\n",
    "            for i in range(len(candidates)):\n",
    "                for j in range(i+1, len(candidates)):\n",
    "                    file1, col1, _ = candidates[i]\n",
    "                    file2, col2, _ = candidates[j]\n",
    "                    if G.has_edge(file1, file2):\n",
    "                        G[file1][file2]['candidate_keys'].append((sig, col1, col2))\n",
    "                    else:\n",
    "                        G.add_edge(file1, file2, candidate_keys=[(sig, col1, col2)])\n",
    "    return G\n",
    "\n",
    "\n",
    "def find_common_join_key(df1, df2, threshold=100):\n",
    "    \"\"\"\n",
    "    Identify a common join key between two DataFrames.\n",
    "    \n",
    "    It computes candidate signatures for each and returns a tuple:\n",
    "      (column_name_in_df1, column_name_in_df2, candidate_signature)\n",
    "    for the first candidate signature that both dataframes share.\n",
    "    If no candidate is found, returns None.\n",
    "    \"\"\"\n",
    "    cand1 = get_candidate_signatures(df1, threshold)\n",
    "    cand2 = get_candidate_signatures(df2, threshold)\n",
    "    common_keys = set(cand1.keys()).intersection(set(cand2.keys()))\n",
    "    if common_keys:\n",
    "        # if several keys are common, choose one based on lower unique count in df1\n",
    "        best_key = None\n",
    "        best_count = None\n",
    "        for key in common_keys:\n",
    "            col1 = cand1[key][0]\n",
    "            col2 = cand2[key][0]\n",
    "            count1 = len(df1[col1].dropna().unique())\n",
    "            if best_count is None or count1 < best_count:\n",
    "                best_count = count1\n",
    "                best_key = (col1, col2, key)\n",
    "        return best_key\n",
    "    return None\n",
    "\n",
    "\n",
    "def join_dataframes(dataframes, candidate_mapping, threshold=100):\n",
    "    \"\"\"\n",
    "    Using the candidate mapping, build a join graph and for each connected component,\n",
    "    iteratively join the dataframes. For each connected component, the file with the fewest\n",
    "    rows (a proxy for lowest resolution) is chosen as the base.\n",
    "    \n",
    "    Returns a dictionary where keys are base file names (or component identifiers) and\n",
    "    values are the merged DataFrames.\n",
    "    \"\"\"\n",
    "    G = build_join_graph(candidate_mapping)\n",
    "    components = list(nx.connected_components(G))\n",
    "    merged_dfs = {}\n",
    "    processed_files = set()\n",
    "    \n",
    "    for comp in components:\n",
    "        if len(comp) == 1:\n",
    "            # No join possible; keep the file as is.\n",
    "            file = list(comp)[0]\n",
    "            merged_dfs[file] = dataframes[file]\n",
    "            processed_files.add(file)\n",
    "        else:\n",
    "            comp_files = list(comp)\n",
    "            # choose base file as one with the smallest number of rows\n",
    "            base_file = min(comp_files, key=lambda f: len(dataframes[f]))\n",
    "            merged_df = dataframes[base_file]\n",
    "            merged_files = {base_file}\n",
    "            remaining_files = set(comp_files) - merged_files\n",
    "\n",
    "            # Iteratively try to join any remaining file with the merged dataframe\n",
    "            while remaining_files:\n",
    "                joined = False\n",
    "                for file in list(remaining_files):\n",
    "                    join_key = find_common_join_key(merged_df, dataframes[file], threshold)\n",
    "                    if join_key:\n",
    "                        col_merged, col_new, sig = join_key\n",
    "                        df_to_join = dataframes[file].copy()\n",
    "                        # if the join column names differ, rename the new dataframe’s column\n",
    "                        if col_new != col_merged:\n",
    "                            df_to_join = df_to_join.rename(columns={col_new: col_merged})\n",
    "                        # outer join to preserve information\n",
    "                        merged_df = pd.merge(merged_df, df_to_join, on=col_merged, how='outer',\n",
    "                                             suffixes=('', '_' + file))\n",
    "                        merged_files.add(file)\n",
    "                        remaining_files.remove(file)\n",
    "                        joined = True\n",
    "                        break\n",
    "                if not joined:\n",
    "                    # If no join key is found for any remaining file, leave them separate.\n",
    "                    break\n",
    "            # Record the merged DataFrame from this component.\n",
    "            merged_dfs[base_file] = merged_df\n",
    "            processed_files.update(comp)\n",
    "    \n",
    "    # For any files not connected in the join graph, add them as individual outputs.\n",
    "    for file in dataframes:\n",
    "        if file not in processed_files:\n",
    "            merged_dfs[file] = dataframes[file]\n",
    "    return merged_dfs\n",
    "\n",
    "\n",
    "def process_csv_folder(folder_path, threshold=100):\n",
    "    \"\"\"\n",
    "    Wrapper function that:\n",
    "      1. Loads all CSV files from the given folder.\n",
    "      2. Analyzes each DataFrame’s columns for candidate join keys.\n",
    "      3. Identifies join relationships and builds a join graph.\n",
    "      4. Chooses a base dataset (lowest resolution) in each joinable group.\n",
    "      5. Performs iterative joins and returns a minimal set of merged DataFrames.\n",
    "      \n",
    "    The 'threshold' parameter controls the maximum unique value count for a column to be\n",
    "    considered as a candidate join column.\n",
    "    \n",
    "    Returns a dictionary of DataFrames keyed by an identifier.\n",
    "    \"\"\"\n",
    "    dataframes = load_excel_files(folder_path)\n",
    "    candidate_mapping = build_candidate_mapping(dataframes, threshold)\n",
    "    merged_dfs = join_dataframes(dataframes, candidate_mapping, threshold)\n",
    "    return merged_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wild_bee_traits\n",
      "wild_bees\n",
      "environmental_factors\n",
      "legend\n",
      "wild_bees_site_month_level\n",
      "wild_bees_traits\n",
      "wild_bees_site_level\n",
      "distance_matrix\n",
      "raw_data_wild_bees\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 23.0 GiB for an array with shape (188, 16426095) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_csv_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/poc_data_and_similar_paper/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 290\u001b[0m, in \u001b[0;36mprocess_csv_folder\u001b[0;34m(folder_path, threshold)\u001b[0m\n\u001b[1;32m    288\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m load_excel_files(folder_path)\n\u001b[1;32m    289\u001b[0m candidate_mapping \u001b[38;5;241m=\u001b[39m build_candidate_mapping(dataframes, threshold)\n\u001b[0;32m--> 290\u001b[0m merged_dfs \u001b[38;5;241m=\u001b[39m \u001b[43mjoin_dataframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merged_dfs\n",
      "Cell \u001b[0;32mIn[4], line 254\u001b[0m, in \u001b[0;36mjoin_dataframes\u001b[0;34m(dataframes, candidate_mapping, threshold)\u001b[0m\n\u001b[1;32m    252\u001b[0m     df_to_join \u001b[38;5;241m=\u001b[39m df_to_join\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{col_new: col_merged})\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# outer join to preserve information\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_to_join\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m merged_files\u001b[38;5;241m.\u001b[39madd(file)\n\u001b[1;32m    257\u001b[0m remaining_files\u001b[38;5;241m.\u001b[39mremove(file)\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/reshape/merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    171\u001b[0m         left_df,\n\u001b[1;32m    172\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/reshape/merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/reshape/merge.py:848\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m _items_overlap_with_suffix(\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuffixes\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m \u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m     left \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(lmgr, axes\u001b[38;5;241m=\u001b[39mlmgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    858\u001b[0m left\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m join_index\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/internals/managers.py:687\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    681\u001b[0m         indexer,\n\u001b[1;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[1;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    693\u001b[0m             ),\n\u001b[1;32m    694\u001b[0m         )\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n\u001b[1;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/internals/managers.py:688\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    681\u001b[0m         indexer,\n\u001b[1;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 688\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n\u001b[1;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/piotr/projects/ai_research_assistant/dev/lib/python3.10/site-packages/pandas/core/array_algos/take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 23.0 GiB for an array with shape (188, 16426095) and data type int64"
     ]
    }
   ],
   "source": [
    "process_csv_folder(folder_path=\"../data/poc_data_and_similar_paper/\", threshold=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
